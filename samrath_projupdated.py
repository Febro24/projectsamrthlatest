# -*- coding: utf-8 -*-
"""samrath_proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15xI22qEb686ypqQ3WFPbaMWW3ji1YtUJ
"""



# from google.colab import drive
# drive.mount('/content/drive')

import pandas as pd
import requests
import io

pd.set_option('display.max_columns', None)

# Step 2: Read Rainfall CSVs
rainfall_allindia = pd.read_csv('D:\samrathproj\Rainfallallindia_AI_1901-2021.csv')
rainfall_subdiv = pd.read_csv('D:\samrathproj\Sub_Divisionalmonthlyrainfall_IMD_2017.csv')

print("All-India Rainfall Data:")
print(rainfall_allindia.head())

print("\nSub-divisional Rainfall Data:")
print(rainfall_subdiv.head())

# Step 3: Fetch data from data.gov.in API
url = "https://api.data.gov.in/resource/35be999b-0208-4354-b557-f6ca9a5355de"
params = {
    "api-key": "579b464db66ec23bdd000001923d57b91b764d344ad3f46f31fb8ebd",   # replace this after registering on data.gov.in
    "format": "json",
    "limit": 1000000                    # adjust limit to fetch more rows
}

response = requests.get(url, params=params)
data = response.json()

# Convert JSON to DataFrame
crop_df = pd.DataFrame(data['records'])
print("Crop Production Data:")
print(crop_df.head())

print("\nTotal records available from the API:")
print(data.get('total'))

print("\nRainfall datasets shapes:")
print(rainfall_allindia.shape, rainfall_subdiv.shape)

print("\nCrop production dataset shape:")
print(crop_df.shape)

print("\nColumn names:")
print(crop_df.columns)

# Step 6: Data Cleaning and Standardization

# Clean column names for consistency
crop_df.columns = crop_df.columns.str.strip().str.lower()

# Convert data types
crop_df['crop_year'] = pd.to_numeric(crop_df['crop_year'], errors='coerce')
crop_df['area_'] = pd.to_numeric(crop_df['area_'], errors='coerce')
crop_df['production_'] = pd.to_numeric(crop_df['production_'], errors='coerce')

# Drop null or invalid years
crop_df = crop_df.dropna(subset=['crop_year'])

# Remove any extra spaces in state/district names
crop_df['state_name'] = crop_df['state_name'].str.strip()
crop_df['district_name'] = crop_df['district_name'].str.strip()
crop_df['season'] = crop_df['season'].str.strip()
crop_df['crop'] = crop_df['crop'].str.strip()

# Check summary
print("‚úÖ Crop dataset cleaned successfully!")
print(crop_df.info())
print("\nSample:")
print(crop_df.head(3))

# Step 7: Clean Rainfall Datasets

# --- All-India Rainfall ---
rainfall_allindia.columns = rainfall_allindia.columns.str.strip().str.lower()
if 'year' not in rainfall_allindia.columns:
    rainfall_allindia.rename(columns={'year.1': 'year'}, inplace=True)

rainfall_allindia['year'] = pd.to_numeric(rainfall_allindia['year'], errors='coerce')
rainfall_allindia = rainfall_allindia.dropna(subset=['year'])

print("‚úÖ All-India rainfall sample:")
print(rainfall_allindia.head(3))

# --- Sub-Divisional Rainfall ---
rainfall_subdiv.columns = rainfall_subdiv.columns.str.strip().str.lower()

if 'year' not in rainfall_subdiv.columns:
    rainfall_subdiv.rename(columns={'year.1': 'year'}, inplace=True)

rainfall_subdiv['year'] = pd.to_numeric(rainfall_subdiv['year'], errors='coerce')
rainfall_subdiv = rainfall_subdiv.dropna(subset=['year'])

print("\n‚úÖ Sub-divisional rainfall sample:")
print(rainfall_subdiv.head(3))
# Step 7: Clean Rainfall Datasets

# --- All-India Rainfall ---
rainfall_allindia.columns = rainfall_allindia.columns.str.strip().str.lower()
if 'year' not in rainfall_allindia.columns:
    rainfall_allindia.rename(columns={'year.1': 'year'}, inplace=True)

rainfall_allindia['year'] = pd.to_numeric(rainfall_allindia['year'], errors='coerce')
rainfall_allindia = rainfall_allindia.dropna(subset=['year'])

print("‚úÖ All-India rainfall sample:")
print(rainfall_allindia.head(3))

# --- Sub-Divisional Rainfall ---
rainfall_subdiv.columns = rainfall_subdiv.columns.str.strip().str.lower()

if 'year' not in rainfall_subdiv.columns:
    rainfall_subdiv.rename(columns={'year.1': 'year'}, inplace=True)

rainfall_subdiv['year'] = pd.to_numeric(rainfall_subdiv['year'], errors='coerce')
rainfall_subdiv = rainfall_subdiv.dropna(subset=['year'])

print("\n‚úÖ Sub-divisional rainfall sample:")
print(rainfall_subdiv.head(3))

# Step 8: Merge crop data with All-India rainfall (year-based)
merged_df = pd.merge(
    crop_df,
    rainfall_allindia[['year', 'jun-sep']],
    left_on='crop_year',
    right_on='year',
    how='inner'
)

# Drop duplicate year column
merged_df.drop('year', axis=1, inplace=True)

print("‚úÖ Merged Dataset Preview:")
print(merged_df.head())

print("\nMerged dataset shape:", merged_df.shape)

# Step 9: Quick EDA
print("Top 5 crops:")
print(merged_df['crop'].value_counts().head())

# Correlation check between rainfall and production
corr = merged_df[['jun-sep', 'production_']].corr()
print("\nüåßÔ∏è Monsoon Rainfall vs üåæ Production correlation:")
print(corr)

# Step 11: Analytical reasoning functions

def compare_rainfall(state1, state2, n_years=5):
    """Compare average annual rainfall for two states for last n years."""
    df = merged_df.copy()
    recent_years = sorted(df['crop_year'].unique())[-n_years:]
    comparison = df[df['crop_year'].isin(recent_years)]

    result = comparison.groupby('state_name')['annual'].mean().reset_index()
    result = result[result['state_name'].isin([state1, state2])]
    return result

def top_crops_by_state(state, n=5):
    """Top crops by total production in a state."""
    df = merged_df[merged_df['state_name'] == state]
    top_crops = df.groupby('crop')['production_'].sum().sort_values(ascending=False).head(n)
    return top_crops

def crop_trend(crop_name, region=None):
    """Analyze production trend of a crop (optionally for a state)."""
    df = merged_df[merged_df['crop'] == crop_name]
    if region:
        df = df[df['state_name'] == region]
    trend = df.groupby('crop_year')[['production_', 'annual']].mean().reset_index()
    return trend

print("‚úÖ Analytical reasoning functions loaded successfully!")

# Step 11: Analytical reasoning functions

def compare_rainfall(state1, state2, n_years=5):
    """Compare average monsoon rainfall for two states for last n years."""
    df = merged_df.copy()
    recent_years = sorted(df['crop_year'].unique())[-n_years:]
    comparison = df[df['crop_year'].isin(recent_years)]

    result = comparison.groupby('state_name')['jun-sep'].mean().reset_index()
    result = result[result['state_name'].isin([state1, state2])]
    return result

def top_crops_by_state(state, n=5):
    """Top crops by total production in a state."""
    df = merged_df[merged_df['state_name'] == state]
    top_crops = df.groupby('crop')['production_'].sum().sort_values(ascending=False).head(n)
    return top_crops

def crop_trend(crop_name, region=None):
    """Analyze production and monsoon rainfall trend of a crop (optionally for a state)."""
    df = merged_df[merged_df['crop'] == crop_name]
    if region:
        df = df[df['state_name'] == region]
    trend = df.groupby('crop_year')[['production_', 'jun-sep']].mean().reset_index()
    return trend

print("‚úÖ Analytical reasoning functions loaded successfully!")

# Compare rainfall
print("Monsoon Rainfall comparison:")
print(compare_rainfall("Maharashtra", "Tamil Nadu", n_years=5))

# Top crops in a state
print("\nTop crops in Punjab:")
print(top_crops_by_state("Punjab", n=5))

# Crop trend over time
print("\nWheat and Monsoon Rainfall trend in Uttar Pradesh:")
print(crop_trend("Wheat", "Uttar Pradesh").head())

import re

def samarth_query(query):
    query = query.lower()

    # Rainfall comparison
    if "compare" in query and "rainfall" in query:
        states = re.findall(r"in (\w+)", query)
        years = re.findall(r"last (\d+)", query)
        if len(states) >= 2:
            s1, s2 = states[:2]
            n = int(years[0]) if years else 5
            return compare_rainfall(s1.title(), s2.title(), n)

    # Top crops
    elif "top" in query and "crops" in query:
        state = re.findall(r"in (\w+)", query)
        if state:
            return top_crops_by_state(state[0].title())

    # Crop trend
    elif "trend" in query or "over years" in query:
        crops = re.findall(r"of (\w+)", query)
        states = re.findall(r"in (\w+)", query)
        if crops:
            crop = crops[0].title()
            state = states[0].title() if states else None
            return crop_trend(crop, state)

    else:
        return "Sorry, I couldn‚Äôt understand your question. Try rephrasing."

print("üß† Samarth Q&A interface ready!")

def plot_crop_trend(crop_name, region=None):
    """Plot rainfall vs crop production over years."""
    trend = crop_trend(crop_name, region)
    if trend.empty:
        print("‚ö†Ô∏è No data available for this combination.")
        return

    fig, ax1 = plt.subplots(figsize=(8,5))

    # Crop production (bar)
    ax1.bar(trend['crop_year'], trend['production_'], alpha=0.6, label='Production (tonnes)')
    ax1.set_ylabel('Production (tonnes)', color='tab:blue')
    ax1.set_xlabel('Year')
    ax1.tick_params(axis='y', labelcolor='tab:blue')

    # Rainfall (line)
    ax2 = ax1.twinx()
    ax2.plot(trend['crop_year'], trend['jun-sep'], color='tab:red', label='Monsoon Rainfall (mm)', marker='o')
    ax2.set_ylabel('Monsoon Rainfall (mm)', color='tab:red')
    ax2.tick_params(axis='y', labelcolor='tab:red')


"""### Train a Linear Regression Model

Let's train a simple linear regression model to predict crop production based on monsoon rainfall.
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Prepare the data
# We'll use 'jun-sep' rainfall as the feature and 'production_' as the target
# Drop rows with missing production or rainfall data for this analysis
model_df = merged_df.dropna(subset=['jun-sep', 'production_']).copy()

X = model_df[['jun-sep']] # Features
y = model_df['production_'] # Target

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define and train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("‚úÖ Linear Regression Model Trained and Evaluated")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

# Now the 'model' variable is defined and can be saved later

import joblib

# Save trained model
joblib.dump(model, "qa_model.pkl")

# Note: A vectorizer was not used in this linear regression model, so it's not being saved.
 #joblib.dump(vectorizer, "vectorizer.pkl")

print("‚úÖ Trained model saved successfully as qa_model.pkl")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
import joblib

# Example: using rainfall and crop dataset combined questions
X = ["rainfall in kerala", "wheat production 2020", "monsoon effect on rice"]
y = ["Rainfall increased", "Production moderate", "Rainfall affected rice"]

# Create pipeline (vectorizer + model)
model = make_pipeline(TfidfVectorizer(), LogisticRegression())
model.fit(X, y)

# Save it
joblib.dump(model, "qa_model.pkl")

joblib.dump(model, "qa_model1.pkl")

"""# Task
Develop an intelligent Q&A system that answers complex questions about India‚Äôs agriculture and climate using the provided datasets.

## Understand user queries

### Subtask:
Develop a mechanism to understand the user's questions, identifying key entities (like crops, states, years) and the type of information requested (e.g., comparison, trend, top items). This might involve using NLP techniques.

**Reasoning**:
The task requires creating a function `understand_query` to parse user input. I will define this function and implement the logic to identify keywords and extract relevant entities using regular expressions as instructed.
"""

